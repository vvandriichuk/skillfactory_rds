{"cells":[{"metadata":{},"cell_type":"markdown","source":"# Car Price prediction\n\n<img src=\"https://whatcar.vn/media/2018/09/car-lot-940x470.jpg\"/>\n\n## Прогнозирование стоимости автомобиля по характеристикам"},{"metadata":{},"cell_type":"markdown","source":"## Cделаем следующее:\n* Построим \"наивную\"/baseline модель, предсказывающую цену по модели и году выпуска (с ней будем сравнивать другие модели)\n* Обработаем и отнормируем признаки\n* Сделаем первую модель на основе градиентного бустинга с помощью CatBoost\n* Сделаем вторую модель на основе нейронных сетей и сравним результаты\n* Сделаем multi-input нейронную сеть для анализа табличных данных и текста одновременно\n* Добавим в multi-input сеть обработку изображений\n* Осуществим ансамблирование градиентного бустинга и нейронной сети (усреднение их предсказаний)"},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"!pip install -q tensorflow==2.3","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"#аугментации изображений\n!pip install albumentations -q","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_kg_hide-input":true,"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport random\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport os\nimport sys\nimport PIL\nimport cv2\nimport re\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nfrom catboost import CatBoostRegressor\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import MinMaxScaler\n\n# # keras\nimport tensorflow as tf\nimport tensorflow.keras.layers as L\nfrom tensorflow.keras.models import Model, Sequential\nfrom tensorflow.keras.preprocessing.text import Tokenizer\nfrom tensorflow.keras.preprocessing import sequence\nfrom tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping\nimport albumentations\n\n# plt\nimport matplotlib.pyplot as plt\n#увеличим дефолтный размер графиков\nfrom pylab import rcParams\nrcParams['figure.figsize'] = 10, 5\n#графики в svg выглядят более четкими\n%config InlineBackend.figure_format = 'svg' \n%matplotlib inline\n\n# You can write up to 5GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"print('Python       :', sys.version.split('\\n')[0])\nprint('Numpy        :', np.__version__)\nprint('Tensorflow   :', tf.__version__)","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"def mape(y_true, y_pred):\n    return np.mean(np.abs((y_pred-y_true)/y_true))","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"# фиксируем RANDOM_SEED, чтобы наши эксперименты были воспроизводимы!\nRANDOM_SEED = 42\nnp.random.seed(RANDOM_SEED)","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"!pip freeze > requirements.txt","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# DATA"},{"metadata":{},"cell_type":"markdown","source":"Посмотрим на типы признаков:\n\n* bodyType - категориальный\n* brand - категориальный\n* color - категориальный\n* description - текстовый\n* engineDisplacement - числовой, представленный как текст\n* enginePower - числовой, представленный как текст\n* fuelType - категориальный\n* mileage - числовой\n* modelDate - числовой\n* model_info - категориальный\n* name - категориальный, желательно сократить размерность\n* numberOfDoors - категориальный\n* price - числовой, целевой\n* productionDate - числовой\n* sell_id - изображение (файл доступен по адресу, основанному на sell_id)\n* vehicleConfiguration - не используется (комбинация других столбцов)\n* vehicleTransmission - категориальный\n* Владельцы - категориальный\n* Владение - числовой, представленный как текст\n* ПТС - категориальный\n* Привод - категориальный\n* Руль - категориальный"},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":true},"cell_type":"code","source":"DATA_DIR = '../input/sf-dst-car-price-prediction-part2/'\ntrain = pd.read_csv(DATA_DIR + 'train.csv')\ntest = pd.read_csv(DATA_DIR + 'test.csv')\nsample_submission = pd.read_csv(DATA_DIR + 'sample_submission.csv')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train.info()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train.nunique()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Model 1: Создадим \"наивную\" модель \nЭта модель будет предсказывать среднюю цену по модели и году выпуска. \nC ней будем сравнивать другие модели.\n\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"# split данных\ndata_train, data_test = train_test_split(train, test_size=0.15, shuffle=True, random_state=RANDOM_SEED)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Наивная модель\npredicts = []\nfor index, row in pd.DataFrame(data_test[['model_info', 'productionDate']]).iterrows():\n    query = f\"model_info == '{row[0]}' and productionDate == '{row[1]}'\"\n    predicts.append(data_train.query(query)['price'].median())\n\n# заполним не найденные совпадения\npredicts = pd.DataFrame(predicts)\npredicts = predicts.fillna(predicts.median())\n\n# округлим\npredicts = (predicts // 1000) * 1000\n\n#оцениваем точность\nprint(f\"Точность наивной модели по метрике MAPE: {(mape(data_test['price'], predicts.values[:, 0]))*100:0.2f}%\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# EDA"},{"metadata":{},"cell_type":"markdown","source":"Проведем быстрый анализ данных для того, чтобы понимать, сможет ли с этими данными работать наш алгоритм."},{"metadata":{},"cell_type":"markdown","source":"Посмотрим, как выглядят распределения числовых признаков:"},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"#посмотрим, как выглядят распределения числовых признаков\ndef visualize_distributions(titles_values_dict):\n  columns = min(3, len(titles_values_dict))\n  rows = (len(titles_values_dict) - 1) // columns + 1\n  fig = plt.figure(figsize = (columns * 6, rows * 4))\n  for i, (title, values) in enumerate(titles_values_dict.items()):\n    hist, bins = np.histogram(values, bins = 20)\n    ax = fig.add_subplot(rows, columns, i + 1)\n    ax.bar(bins[:-1], hist, width = (bins[1] - bins[0]) * 0.7)\n    ax.set_title(title)\n  plt.show()\n\nvisualize_distributions({\n    'mileage': train['mileage'].dropna(),\n    'modelDate': train['modelDate'].dropna(),\n    'productionDate': train['productionDate'].dropna()\n})","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Итого:\n* CatBoost сможет работать с признаками и в таком виде, но для нейросети нужны нормированные данные."},{"metadata":{"trusted":true},"cell_type":"code","source":"train['engineDisplacement'] = train['engineDisplacement'].str.split().apply(lambda s: s[0]) \n\ntrain.loc[train.engineDisplacement=='undefined', 'engineDisplacement'] = '2.0' \ntrain.loc[train.engineDisplacement=='0.7', 'engineDisplacement'] = '1.0' \ntrain['engineDisplacement'] = train['engineDisplacement'].apply(lambda s: float(s)) \n\ntrain['enginePower'] = train['enginePower'].str.split().apply(lambda s: s[0]) \ntrain['enginePower'] = train['enginePower'].apply(lambda s: float(s))\n\ntrain['modelDate']=2021-train['modelDate'] \n\ntrain.drop(['name', 'vehicleConfiguration', 'Владение', 'modelDate' ], axis=1, inplace=True,)\n\n \ntrain['productionDate']=2021-train['productionDate']\n\ntrain.loc[train['Владельцы'].isna()==True, 'Владельцы'] = '3 или более'\n\n\n#mileagePerYear - сделаем новый признак - пробег в год \ntrain['mileagePerYear']=train['mileage']/train['productionDate']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test['engineDisplacement'] = test['engineDisplacement'].str.split().apply(lambda s: s[0]) \ntest.loc[test.engineDisplacement=='undefined', 'engineDisplacement'] = '2.0' \ntest.loc[test.engineDisplacement=='0.7', 'engineDisplacement'] = '1.0' \ntest['engineDisplacement'] = test['engineDisplacement'].apply(lambda s: float(s)) \n\ntest['enginePower'] = test['enginePower'].str.split().apply(lambda s: s[0]) \ntest['enginePower'] = test['enginePower'].apply(lambda s: float(s))\n\ntest['modelDate']=2021-test['modelDate'] \n\ntest.drop(['name', 'vehicleConfiguration','Владение', 'modelDate' ], axis=1, inplace=True,)\n\n \ntest['productionDate']=2021-test['productionDate']\n\ntest.loc[train['Владельцы'].isna()==True, 'Владельцы'] = '3 или более'\n\n\n#mileagePerYear - сделаем новый признак - пробег в год \ntest['mileagePerYear']=test['mileage']/test['productionDate']","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# PreProc Tabular Data"},{"metadata":{"trusted":true},"cell_type":"code","source":"#используем текстовые признаки как категориальные\n\ncategorical_features = ['bodyType', 'brand', 'color', 'Руль', 'fuelType', 'model_info', 'vehicleTransmission', 'Владельцы', 'ПТС', 'Привод']\n\n#используем числовые признаки \n\nnumerical_features = ['mileage', 'productionDate','engineDisplacement', 'enginePower', 'numberOfDoors','mileagePerYear']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# ВАЖНО! дря корректной обработки признаков объединяем трейн и тест в один датасет\ntrain['sample'] = 1 # помечаем где у нас трейн\ntest['sample'] = 0 # помечаем где у нас тест\ntest['price'] = 0 # в тесте у нас нет значения price, мы его должны предсказать, поэтому пока просто заполняем нулями\n\ndata = test.append(train, sort=False).reset_index(drop=True) # объединяем\nprint(train.shape, test.shape, data.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def preproc_data(df_input):\n    '''includes several functions to pre-process the predictor data.'''\n    \n    df_output = df_input.copy()\n    \n    df_output.drop(['description', 'sell_id'], axis = 1, inplace=True)\n    \n    # Нормализация данных\n    scaler = MinMaxScaler()\n    for column in numerical_features:\n        df_output[column] = scaler.fit_transform(df_output[[column]])[:,0]\n    \n    # Label Encoding\n    for column in categorical_features:\n        df_output[column] = df_output[column].astype('category').cat.codes\n        \n    # One-Hot Encoding: в pandas есть готовая функция - get_dummies.\n    df_output = pd.get_dummies(df_output, columns=categorical_features, dummy_na=False)\n    \n    return df_output","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Запускаем и проверяем, что получилось\ndf_preproc = preproc_data(data)\ndf_preproc.sample(10)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_preproc.info()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Split data"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Теперь выделим тестовую часть\ntrain_data = df_preproc.query('sample == 1').drop(['sample'], axis=1)\ntest_data = df_preproc.query('sample == 0').drop(['sample'], axis=1)\n\ny = train_data.price.values     # наш таргет\nX = train_data.drop(['price'], axis=1)\nX_sub = test_data.drop(['price'], axis=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test_data.info()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Model 2: CatBoostRegressor"},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.15, shuffle=True, random_state=RANDOM_SEED)","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-output":true,"trusted":true},"cell_type":"code","source":"model = CatBoostRegressor(iterations = 6000,\n                          #depth=10,\n                          #learning_rate = 0.5,\n                          random_seed = RANDOM_SEED,\n                          eval_metric='MAPE',\n                          custom_metric=['RMSE', 'MAE'],\n                          od_wait=500,\n                          #task_type='GPU',\n                         )\nmodel.fit(X_train, y_train,\n         eval_set=(X_test, y_test),\n         verbose_eval=100,\n         use_best_model=True,\n         #plot=True\n         )","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test_predict_catboost = model.predict(X_test)\nprint(f\"TEST mape: {(mape(y_test, test_predict_catboost))*100:0.2f}%\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Submission"},{"metadata":{"trusted":true},"cell_type":"code","source":"sub_predict_catboost = model.predict(X_sub)\nsample_submission['price'] = sub_predict_catboost\nsample_submission.to_csv('catboost_submission.csv', index=False)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Model 3: Tabular NN"},{"metadata":{},"cell_type":"markdown","source":"Построим обычную сеть:"},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train.head(5)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Simple Dense NN"},{"metadata":{"trusted":true},"cell_type":"code","source":"model = Sequential()\nmodel.add(L.Dense(512, input_dim=X_train.shape[1], activation=\"relu\"))\nmodel.add(L.Dropout(0.5))\nmodel.add(L.Dense(256, activation=\"relu\"))\nmodel.add(L.Dropout(0.5))\nmodel.add(L.Dense(1, activation=\"linear\"))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model.summary()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Compile model\noptimizer = tf.keras.optimizers.Adam(0.01)\nmodel.compile(loss='MAPE',optimizer=optimizer, metrics=['MAPE'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"checkpoint = ModelCheckpoint('../working/best_model.hdf5' , monitor=['val_MAPE'], verbose=0  , mode='min')\nearlystop = EarlyStopping(monitor='val_MAPE', patience=50, restore_best_weights=True,)\ncallbacks_list = [checkpoint, earlystop]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Fit"},{"metadata":{"trusted":true},"cell_type":"code","source":"history = model.fit(X_train, y_train,\n                    batch_size=512,\n                    epochs=500, # фактически мы обучаем пока EarlyStopping не остановит обучение\n                    validation_data=(X_test, y_test),\n                    callbacks=callbacks_list,\n                    verbose=0,\n                   )","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.title('Loss')\nplt.plot(history.history['MAPE'], label='train')\nplt.plot(history.history['val_MAPE'], label='test')\nplt.show();","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model.load_weights('../working/best_model.hdf5')\nmodel.save('../working/nn_1.hdf5')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test_predict_nn1 = model.predict(X_test)\nprint(f\"TEST mape: {(mape(y_test, test_predict_nn1[:,0]))*100:0.2f}%\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sub_predict_nn1 = model.predict(X_sub)\nsample_submission['price'] = sub_predict_nn1[:,0]\nsample_submission.to_csv('nn1_submission.csv', index=False)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Рекомендации для улучшения Model 3:    \n* В нейросеть желательно подавать данные с распределением, близким к нормальному, поэтому от некоторых числовых признаков имеет смысл взять логарифм перед нормализацией. Пример:\n`modelDateNorm = np.log(2020 - data['modelDate'])`\nСтатья по теме: https://habr.com/ru/company/ods/blog/325422\n\n* Извлечение числовых значений из текста:\nПарсинг признаков 'engineDisplacement', 'enginePower', 'Владение' для извлечения числовых значений.\n\n* Cокращение размерности категориальных признаков\nПризнак name 'name' содержит данные, которые уже есть в других столбцах ('enginePower', 'engineDisplacement', 'vehicleTransmission'), поэтому эти данные можно удалить. Затем следует еще сильнее сократить размерность, например, выделив наличие xDrive в качестве отдельного признака."},{"metadata":{},"cell_type":"markdown","source":"# Model 4: NLP + Multiple Inputs"},{"metadata":{"trusted":true},"cell_type":"code","source":"data.description","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# TOKENIZER\n# The maximum number of words to be used. (most frequent)\nMAX_WORDS = 100000\n# Max number of words in each complaint.\nMAX_SEQUENCE_LENGTH = 256","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# split данных\ntext_train = data.description.iloc[X_train.index]\ntext_test = data.description.iloc[X_test.index]\ntext_sub = data.description.iloc[X_sub.index]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Tokenizer"},{"metadata":{"trusted":true},"cell_type":"code","source":"%%time\ntokenize = Tokenizer(num_words=MAX_WORDS)\ntokenize.fit_on_texts(data.description)","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-output":true,"trusted":true},"cell_type":"code","source":"tokenize.word_index","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"%%time\ntext_train_sequences = sequence.pad_sequences(tokenize.texts_to_sequences(text_train), maxlen=MAX_SEQUENCE_LENGTH)\ntext_test_sequences = sequence.pad_sequences(tokenize.texts_to_sequences(text_test), maxlen=MAX_SEQUENCE_LENGTH)\ntext_sub_sequences = sequence.pad_sequences(tokenize.texts_to_sequences(text_sub), maxlen=MAX_SEQUENCE_LENGTH)\n\nprint(text_train_sequences.shape, text_test_sequences.shape, text_sub_sequences.shape, )","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# вот так теперь выглядит наш текст\nprint(text_train.iloc[6])\nprint(text_train_sequences[6])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### RNN NLP"},{"metadata":{"trusted":true},"cell_type":"code","source":"model_nlp = Sequential()\nmodel_nlp.add(L.Input(shape=MAX_SEQUENCE_LENGTH, name=\"seq_description\"))\nmodel_nlp.add(L.Embedding(len(tokenize.word_index)+1, MAX_SEQUENCE_LENGTH,))\nmodel_nlp.add(L.LSTM(256, return_sequences=True))\nmodel_nlp.add(L.Dropout(0.5))\nmodel_nlp.add(L.LSTM(128,))\nmodel_nlp.add(L.Dropout(0.25))\nmodel_nlp.add(L.Dense(64, activation=\"relu\"))\nmodel_nlp.add(L.Dropout(0.25))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### MLP"},{"metadata":{"trusted":true},"cell_type":"code","source":"model_mlp = Sequential()\nmodel_mlp.add(L.Dense(512, input_dim=X_train.shape[1], activation=\"relu\"))\nmodel_mlp.add(L.Dropout(0.5))\nmodel_mlp.add(L.Dense(256, activation=\"relu\"))\nmodel_mlp.add(L.Dropout(0.5))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Multiple Inputs NN"},{"metadata":{"trusted":true},"cell_type":"code","source":"combinedInput = L.concatenate([model_nlp.output, model_mlp.output])\n# being our regression head\nhead = L.Dense(64, activation=\"relu\")(combinedInput)\nhead = L.Dense(1, activation=\"linear\")(head)\n\nmodel = Model(inputs=[model_nlp.input, model_mlp.input], outputs=head)","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-output":true,"trusted":true},"cell_type":"code","source":"model.summary()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Fit"},{"metadata":{"trusted":true},"cell_type":"code","source":"optimizer = tf.keras.optimizers.Adam(0.01)\nmodel.compile(loss='MAPE',optimizer=optimizer, metrics=['MAPE'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"checkpoint = ModelCheckpoint('../working/best_model.hdf5', monitor=['val_MAPE'], verbose=0, mode='min')\nearlystop = EarlyStopping(monitor='val_MAPE', patience=10, restore_best_weights=True,)\ncallbacks_list = [checkpoint, earlystop]","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-output":true,"trusted":true},"cell_type":"code","source":"history = model.fit([text_train_sequences, X_train], y_train,\n                    batch_size=512,\n                    epochs=500, # фактически мы обучаем пока EarlyStopping не остановит обучение\n                    validation_data=([text_test_sequences, X_test], y_test),\n                    callbacks=callbacks_list\n                   )","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.title('Loss')\nplt.plot(history.history['MAPE'], label='train')\nplt.plot(history.history['val_MAPE'], label='test')\nplt.show();","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model.load_weights('../working/best_model.hdf5')\nmodel.save('../working/nn_mlp_nlp.hdf5')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test_predict_nn2 = model.predict([text_test_sequences, X_test])\nprint(f\"TEST mape: {(mape(y_test, test_predict_nn2[:,0]))*100:0.2f}%\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sub_predict_nn2 = model.predict([text_sub_sequences, X_sub])\nsample_submission['price'] = sub_predict_nn2[:,0]\nsample_submission.to_csv('nn2_submission.csv', index=False)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Идеи для улучшения NLP части:\n* Выделить из описаний часто встречающиеся блоки текста, заменив их на кодовые слова или удалив\n* Сделать предобработку текста, например, сделать лемматизацию - алгоритм ставящий все слова в форму по умолчанию (глаголы в инфинитив и т. д.), чтобы токенайзер не преобразовывал разные формы слова в разные числа\nСтатья по теме: https://habr.com/ru/company/Voximplant/blog/446738/\n* Поработать над алгоритмами очистки и аугментации текста"},{"metadata":{},"cell_type":"markdown","source":"# Model 5: Добавляем картинки"},{"metadata":{},"cell_type":"markdown","source":"### Data"},{"metadata":{"trusted":true},"cell_type":"code","source":"# убедимся, что цены и фото подгрузились верно\nplt.figure(figsize = (12,8))\n\nrandom_image = train.sample(n = 9)\nrandom_image_paths = random_image['sell_id'].values\nrandom_image_cat = random_image['price'].values\n\nfor index, path in enumerate(random_image_paths):\n    im = PIL.Image.open(DATA_DIR+'img/img/' + str(path) + '.jpg')\n    plt.subplot(3, 3, index + 1)\n    plt.imshow(im)\n    plt.title('price: ' + str(random_image_cat[index]))\n    plt.axis('off')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"size = (320, 240)\n\ndef get_image_array(index):\n    images_train = []\n    for index, sell_id in enumerate(data['sell_id'].iloc[index].values):\n        image = cv2.imread(DATA_DIR + 'img/img/' + str(sell_id) + '.jpg')\n        assert(image is not None)\n        image = cv2.resize(image, size)\n        images_train.append(image)\n    images_train = np.array(images_train)\n    print('images shape', images_train.shape, 'dtype', images_train.dtype)\n    return(images_train)\n\nimages_train = get_image_array(X_train.index)\nimages_test = get_image_array(X_test.index)\nimages_sub = get_image_array(X_sub.index)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### albumentations"},{"metadata":{"trusted":true},"cell_type":"code","source":"from albumentations import (\n    HorizontalFlip, IAAPerspective, ShiftScaleRotate, CLAHE, RandomRotate90,\n    Transpose, ShiftScaleRotate, Blur, OpticalDistortion, GridDistortion, HueSaturationValue,\n    IAAAdditiveGaussianNoise, GaussNoise, MotionBlur, MedianBlur, IAAPiecewiseAffine,\n    IAASharpen, IAAEmboss, RandomBrightnessContrast, Flip, OneOf, Compose\n)\n\n\n#пример взят из официальной документации: https://albumentations.readthedocs.io/en/latest/examples.html\naugmentation = Compose([\n    HorizontalFlip(),\n    OneOf([\n        IAAAdditiveGaussianNoise(),\n        GaussNoise(),\n    ], p=0.2),\n    OneOf([\n        MotionBlur(p=0.2),\n        MedianBlur(blur_limit=3, p=0.1),\n        Blur(blur_limit=3, p=0.1),\n    ], p=0.2),\n    ShiftScaleRotate(shift_limit=0.0625, scale_limit=0.2, rotate_limit=15, p=1),\n    OneOf([\n        OpticalDistortion(p=0.3),\n        GridDistortion(p=0.1),\n        IAAPiecewiseAffine(p=0.3),\n    ], p=0.2),\n    OneOf([\n        CLAHE(clip_limit=2),\n        IAASharpen(),\n        IAAEmboss(),\n        RandomBrightnessContrast(),\n    ], p=0.3),\n    HueSaturationValue(p=0.3),\n], p=1)\n\n#пример\nplt.figure(figsize = (12,8))\nfor i in range(9):\n    img = augmentation(image = images_train[0])['image']\n    plt.subplot(3, 3, i + 1)\n    plt.imshow(img)\n    plt.axis('off')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## tf.data.Dataset\nЕсли все изображения мы будем хранить в памяти, то может возникнуть проблема ее нехватки. Не храните все изображения в памяти целиком!\n\nМетод .fit() модели keras может принимать либо данные в виде массивов или тензоров, либо разного рода итераторы, из которых наиболее современным и гибким является [tf.data.Dataset](https://www.tensorflow.org/guide/data). Он представляет собой конвейер, то есть мы указываем, откуда берем данные и какую цепочку преобразований с ними выполняем. Далее мы будем работать с tf.data.Dataset.\n\nDataset хранит информацию о конечном или бесконечном наборе кортежей (tuple) с данными и может возвращать эти наборы по очереди. Например, данными могут быть пары (input, target) для обучения нейросети. С данными можно осуществлять преобразования, которые осуществляются по мере необходимости ([lazy evaluation](https://ru.wikipedia.org/wiki/%D0%9B%D0%B5%D0%BD%D0%B8%D0%B2%D1%8B%D0%B5_%D0%B2%D1%8B%D1%87%D0%B8%D1%81%D0%BB%D0%B5%D0%BD%D0%B8%D1%8F)).\n\n`tf.data.Dataset.from_tensor_slices(data)` - создает датасет из данных, которые представляют собой либо массив, либо кортеж из массивов. Деление осуществляется по первому индексу каждого массива. Например, если `data = (np.zeros((128, 256, 256)), np.zeros(128))`, то датасет будет содержать 128 элементов, каждый из которых содержит один массив 256x256 и одно число.\n\n`dataset2 = dataset1.map(func)` - применение функции к датасету; функция должна принимать столько аргументов, каков размер кортежа в датасете 1 и возвращать столько, сколько нужно иметь в датасете 2. Пусть, например, датасет содержит изображения и метки, а нам нужно создать датасет только из изображений, тогда мы напишем так: `dataset2 = dataset.map(lambda img, label: img)`.\n\n`dataset2 = dataset1.batch(8)` - группировка по батчам; если датасет 2 должен вернуть один элемент, то он берет из датасета 1 восемь элементов, склеивает их (нулевой индекс результата - номер элемента) и возвращает.\n\n`dataset.__iter__()` - превращение датасета в итератор, из которого можно получать элементы методом `.__next__()`. Итератор, в отличие от самого датасета, хранит позицию текущего элемента. Можно также перебирать датасет циклом for.\n\n`dataset2 = dataset1.repeat(X)` - датасет 2 будет повторять датасет 1 X раз.\n\nЕсли нам нужно взять из датасета 1000 элементов и использовать их как тестовые, а остальные как обучающие, то мы напишем так:\n\n`test_dataset = dataset.take(1000)\ntrain_dataset = dataset.skip(1000)`\n\nДатасет по сути неизменен: такие операции, как map, batch, repeat, take, skip никак не затрагивают оригинальный датасет. Если датасет хранит элементы [1, 2, 3], то выполнив 3 раза подряд функцию dataset.take(1) мы получим 3 новых датасета, каждый из которых вернет число 1. Если же мы выполним функцию dataset.skip(1), мы получим датасет, возвращающий числа [2, 3], но исходный датасет все равно будет возвращать [1, 2, 3] каждый раз, когда мы его перебираем.\n\ntf.Dataset всегда выполняется в graph-режиме (в противоположность eager-режиму), поэтому либо преобразования (`.map()`) должны содержать только tensorflow-функции, либо мы должны использовать tf.py_function в качестве обертки для функций, вызываемых в `.map()`. Подробнее можно прочитать [здесь](https://www.tensorflow.org/guide/data#applying_arbitrary_python_logic)."},{"metadata":{"trusted":true},"cell_type":"code","source":"# NLP part\ntokenize = Tokenizer(num_words=MAX_WORDS)\ntokenize.fit_on_texts(data.description)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def process_image(image):\n    return augmentation(image = image.numpy())['image']\n\ndef tokenize_(descriptions):\n  return sequence.pad_sequences(tokenize.texts_to_sequences(descriptions), maxlen = MAX_SEQUENCE_LENGTH)\n\ndef tokenize_text(text):\n    return tokenize_([text.numpy().decode('utf-8')])[0]\n\ndef tf_process_train_dataset_element(image, table_data, text, price):\n    im_shape = image.shape\n    [image,] = tf.py_function(process_image, [image], [tf.uint8])\n    image.set_shape(im_shape)\n    [text,] = tf.py_function(tokenize_text, [text], [tf.int32])\n    return (image, table_data, text), price\n\ndef tf_process_val_dataset_element(image, table_data, text, price):\n    [text,] = tf.py_function(tokenize_text, [text], [tf.int32])\n    return (image, table_data, text), price\n\ntrain_dataset = tf.data.Dataset.from_tensor_slices((\n    images_train, X_train, data.description.iloc[X_train.index], y_train\n    )).map(tf_process_train_dataset_element)\n\ntest_dataset = tf.data.Dataset.from_tensor_slices((\n    images_test, X_test, data.description.iloc[X_test.index], y_test\n    )).map(tf_process_val_dataset_element)\n\ny_sub = np.zeros(len(X_sub))\nsub_dataset = tf.data.Dataset.from_tensor_slices((\n    images_sub, X_sub, data.description.iloc[X_sub.index], y_sub\n    )).map(tf_process_val_dataset_element)\n\n#проверяем, что нет ошибок (не будет выброшено исключение):\ntrain_dataset.__iter__().__next__();\ntest_dataset.__iter__().__next__();\nsub_dataset.__iter__().__next__();","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Строим сверточную сеть для анализа изображений без \"головы\""},{"metadata":{"trusted":true},"cell_type":"code","source":"#нормализация включена в состав модели EfficientNetB6, поэтому на вход она принимает данные типа uint8\nefficientnet_model = tf.keras.applications.efficientnet.EfficientNetB6(weights = 'imagenet', include_top = False, input_shape = (size[1], size[0], 3))\nefficientnet_output = L.GlobalAveragePooling2D()(efficientnet_model.output)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#строим нейросеть для анализа табличных данных\ntabular_model = Sequential([\n    L.Input(shape = X.shape[1]),\n    L.Dense(512, activation = 'relu'),\n    L.Dropout(0.5),\n    L.Dense(256, activation = 'relu'),\n    L.Dropout(0.5),\n    ])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# NLP\nnlp_model = Sequential([\n    L.Input(shape=MAX_SEQUENCE_LENGTH, name=\"seq_description\"),\n    L.Embedding(len(tokenize.word_index)+1, MAX_SEQUENCE_LENGTH,),\n    L.LSTM(256, return_sequences=True),\n    L.Dropout(0.5),\n    L.LSTM(128),\n    L.Dropout(0.25),\n    L.Dense(64),\n    ])","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-output":true,"trusted":true},"cell_type":"code","source":"#объединяем выходы трех нейросетей\ncombinedInput = L.concatenate([efficientnet_output, tabular_model.output, nlp_model.output])\n\n# being our regression head\nhead = L.Dense(256, activation=\"relu\")(combinedInput)\nhead = L.Dense(1,)(head)\n\nmodel = Model(inputs=[efficientnet_model.input, tabular_model.input, nlp_model.input], outputs=head)\nmodel.summary()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"optimizer = tf.keras.optimizers.Adam(0.05)\nmodel.compile(loss='MAPE',optimizer=optimizer, metrics=['MAPE'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"checkpoint = ModelCheckpoint('../working/best_model.hdf5', monitor=['val_MAPE'], verbose=0, mode='min')\nearlystop = EarlyStopping(monitor='val_MAPE', patience=10, restore_best_weights=True,)\ncallbacks_list = [checkpoint, earlystop]","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-output":true,"trusted":true},"cell_type":"code","source":"history = model.fit(train_dataset.batch(20),\n                    epochs=60,\n                    validation_data = test_dataset.batch(20),\n                    callbacks=callbacks_list\n                   )","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.title('Loss')\nplt.plot(history.history['MAPE'], label='train')\nplt.plot(history.history['val_MAPE'], label='test')\nplt.show();","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model.load_weights('../working/best_model.hdf5')\nmodel.save('../working/nn_final.hdf5')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test_predict_nn3 = model.predict(test_dataset.batch(30))\nprint(f\"TEST mape: {(mape(y_test, test_predict_nn3[:,0]))*100:0.2f}%\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sub_predict_nn3 = model.predict(sub_dataset.batch(30))\nsample_submission['price'] = sub_predict_nn3[:,0]\nsample_submission.to_csv('nn3_submission.csv', index=False)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"\n#### Общие рекомендации:\n* Попробовать разные архитектуры\n* Провести более детальный анализ результатов\n* Попробовать различные подходы в управление LR и оптимизаторы\n* Поработать с таргетом\n* Использовать Fine-tuning\n\n#### Tabular\n* В нейросеть желательно подавать данные с распределением, близким к нормальному, поэтому от некоторых числовых признаков имеет смысл взять логарифм перед нормализацией. Пример:\n`modelDateNorm = np.log(2020 - data['modelDate'])`\nСтатья по теме: https://habr.com/ru/company/ods/blog/325422\n\n* Извлечение числовых значений из текста:\nПарсинг признаков 'engineDisplacement', 'enginePower', 'Владение' для извлечения числовых значений.\n\n* Cокращение размерности категориальных признаков\nПризнак name 'name' содержит данные, которые уже есть в других столбцах ('enginePower', 'engineDisplacement', 'vehicleTransmission'). Можно удалить эти данные. Затем можно еще сильнее сократить размерность, например выделив наличие xDrive в качестве отдельного признака.\n\n* Поработать над Feature engineering\n\n\n\n#### NLP\n* Выделить из описаний часто встречающиеся блоки текста, заменив их на кодовые слова или удалив\n* Сделать предобработку текста, например сделать лемматизацию - алгоритм ставящий все слова в форму по умолчанию (глаголы в инфинитив и т. д.), чтобы токенайзер не преобразовывал разные формы слова в разные числа\nСтатья по теме: https://habr.com/ru/company/Voximplant/blog/446738/\n* Поработать над алгоритмами очистки и аугментации текста\n\n\n\n#### CV\n* Попробовать различные аугментации\n* Fine-tuning"},{"metadata":{},"cell_type":"markdown","source":"# Blend"},{"metadata":{"trusted":true},"cell_type":"code","source":"blend_predict = (test_predict_catboost + test_predict_nn3[:,0]) / 2\nprint(f\"TEST mape: {(mape(y_test, blend_predict))*100:0.2f}%\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"blend_sub_predict = (sub_predict_catboost + sub_predict_nn3[:,0]) / 2\nsample_submission['price'] = blend_sub_predict\nsample_submission.to_csv('blend_submission.csv', index=False)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Model Bonus: проброс признака"},{"metadata":{"trusted":true},"cell_type":"code","source":"# MLP\nmodel_mlp = Sequential()\nmodel_mlp.add(L.Dense(512, input_dim=X_train.shape[1], activation=\"relu\"))\nmodel_mlp.add(L.Dropout(0.5))\nmodel_mlp.add(L.Dense(256, activation=\"relu\"))\nmodel_mlp.add(L.Dropout(0.5))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# FEATURE Input\n# Iput\nproductiondate = L.Input(shape=[1], name=\"productiondate\")\n# Embeddings layers\nemb_productiondate = L.Embedding(len(X.productionDate.unique().tolist())+1, 20)(productiondate)\nf_productiondate = L.Flatten()(emb_productiondate)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"combinedInput = L.concatenate([model_mlp.output, f_productiondate,])\n# being our regression head\nhead = L.Dense(64, activation=\"relu\")(combinedInput)\nhead = L.Dense(1, activation=\"linear\")(head)\n\nmodel = Model(inputs=[model_mlp.input, productiondate], outputs=head)","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-output":true,"trusted":true},"cell_type":"code","source":"model.summary()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"optimizer = tf.keras.optimizers.Adam(0.01)\nmodel.compile(loss='MAPE',optimizer=optimizer, metrics=['MAPE'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"history = model.fit([X_train, X_train.productionDate.values], y_train,\n                    batch_size=512,\n                    epochs=500, # фактически мы обучаем пока EarlyStopping не остановит обучение\n                    validation_data=([X_test, X_test.productionDate.values], y_test),\n                    callbacks=callbacks_list\n                   )","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model.load_weights('../working/best_model.hdf5')\ntest_predict_nn_bonus = model.predict([X_test, X_test.productionDate.values])\nprint(f\"TEST mape: {(mape(y_test, test_predict_nn_bonus[:,0]))*100:0.2f}%\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}